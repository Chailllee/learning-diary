---
title: "Week7 - Classification-I"
---

## Overfiting

-   Tree score = SSR + tree penalty (alpha) \* T (number of leaves)

对于Tree score公式，为啥去掉越多leaf, alpha越大？

在决策树的剪枝过程中，我们使用一个参数称为**ccp_alpha**（cost complexity parameter）来控制剪枝的程度。这个参数的值越大，意味着我们更倾向于剪掉更多的叶节点，从而简化树的结构。

让我们来理解一下为什么去掉越多的叶节点，**ccp_alpha** 就越大：

1.  **基尼杂质和树的复杂度**：

    -   基尼杂质是用来衡量数据集纯度的指标，它越小表示数据集的纯度越高。

    -   在剪枝过程中，我们希望保留那些对模型性能有积极影响的叶节点，同时减少模型的复杂度。

    -   当我们去掉一个叶节点时，基尼杂质会增加，因为我们丧失了这个叶节点的纯度。

2.  **成本复杂度**：

**ccp_alpha** 是一个成本复杂度参数，它在剪枝过程中平衡了模型的拟合程度和复杂度。

当我们增大 **ccp_alpha** 时，模型更倾向于剪掉更多的叶节点，从而减少模型的复杂度。

这样做的目的是防止过拟合，提高模型的泛化能力。

Youtube video

[How to Prune Regression Trees](https://www.youtube.com/watch?v=D0efHEJsfHo&t=684s "How to Prune Regression Trees")

## Random Forest
