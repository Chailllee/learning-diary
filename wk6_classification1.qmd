---
title: "Week7 - Classification-I"
---

## Overfitting

$$Tree score = SSR + TreePenalty_{alpha}* T_{Mumber Of Leaves}$$

For the Tree score formula, why does removing more leaves result in a larger alpha?

During the pruning process of decision trees, we use a parameter called ccp_alpha (cost complexity parameter) to control the extent of pruning. A higher value of this parameter indicates a greater tendency to remove more leaf nodes, thereby simplifying the tree structure.

Let's understand why removing more leaf nodes leads to a larger ccp_alpha:

### Gini Impurity and Tree Complexity:

Gini impurity is a metric used to measure the purity of a dataset, with smaller values indicating higher purity.

During pruning, we aim to retain leaf nodes that positively contribute to the model's performance while reducing the complexity of the model.

When we remove a leaf node, the Gini impurity increases because we lose the purity associated with that leaf node.

### Cost Complexity:

ccp_alpha is a cost complexity parameter that balances the fit and complexity of the model during pruning.

By increasing ccp_alpha, the model tends to remove more leaf nodes, thereby reducing the complexity of the model.

The purpose of this is to prevent overfitting and improve the generalization ability of the model.
